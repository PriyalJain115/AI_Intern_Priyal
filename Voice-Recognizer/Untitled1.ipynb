{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c29b254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\tf\\lib\\site-packages\\torchaudio\\backend\\utils.py:62: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from typing import Callable, List\n",
    "import torch.nn.functional as F\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1ee508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['ru', 'en', 'de', 'es']\n",
    "\n",
    "\n",
    "class OnnxWrapper():\n",
    "\n",
    "    def __init__(self, path, force_onnx_cpu=False):\n",
    "        import numpy as np\n",
    "        global np\n",
    "        import onnxruntime\n",
    "\n",
    "        opts = onnxruntime.SessionOptions()\n",
    "        opts.inter_op_num_threads = 1\n",
    "        opts.intra_op_num_threads = 1\n",
    "\n",
    "        if force_onnx_cpu and 'CPUExecutionProvider' in onnxruntime.get_available_providers():\n",
    "            self.session = onnxruntime.InferenceSession(path, providers=['CPUExecutionProvider'], sess_options=opts)\n",
    "        else:\n",
    "            self.session = onnxruntime.InferenceSession(path, sess_options=opts)\n",
    "\n",
    "        self.reset_states()\n",
    "        self.sample_rates = [8000, 16000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca7b900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_input(self, x, sr: int):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        if x.dim() > 2:\n",
    "            raise ValueError(f\"Too many dimensions for input audio chunk {x.dim()}\")\n",
    "\n",
    "        if sr != 16000 and (sr % 16000 == 0):\n",
    "            step = sr // 16000\n",
    "            x = x[:,::step]\n",
    "            sr = 16000\n",
    "\n",
    "        if sr not in self.sample_rates:\n",
    "            raise ValueError(f\"Supported sampling rates: {self.sample_rates} (or multiply of 16000)\")\n",
    "\n",
    "        if sr / x.shape[1] > 31.25:\n",
    "            raise ValueError(\"Input audio chunk is too short\")\n",
    "\n",
    "        return x, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7198b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_states(self, batch_size=1):\n",
    "        self._h = np.zeros((2, batch_size, 64)).astype('float32')\n",
    "        self._c = np.zeros((2, batch_size, 64)).astype('float32')\n",
    "        self._last_sr = 0\n",
    "        self._last_batch_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4506bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __call__(self, x, sr: int):\n",
    "\n",
    "        x, sr = self._validate_input(x, sr)\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        if not self._last_batch_size:\n",
    "            self.reset_states(batch_size)\n",
    "        if (self._last_sr) and (self._last_sr != sr):\n",
    "            self.reset_states(batch_size)\n",
    "        if (self._last_batch_size) and (self._last_batch_size != batch_size):\n",
    "            self.reset_states(batch_size)\n",
    "\n",
    "        if sr in [8000, 16000]:\n",
    "            ort_inputs = {'input': x.numpy(), 'h': self._h, 'c': self._c, 'sr': np.array(sr, dtype='int64')}\n",
    "            ort_outs = self.session.run(None, ort_inputs)\n",
    "            out, self._h, self._c = ort_outs\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        self._last_sr = sr\n",
    "        self._last_batch_size = batch_size\n",
    "\n",
    "        out = torch.tensor(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be7c2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_forward(self, x, sr: int, num_samples: int = 512):\n",
    "        outs = []\n",
    "        x, sr = self._validate_input(x, sr)\n",
    "\n",
    "        if x.shape[1] % num_samples:\n",
    "            pad_num = num_samples - (x.shape[1] % num_samples)\n",
    "            x = torch.nn.functional.pad(x, (0, pad_num), 'constant', value=0.0)\n",
    "\n",
    "        self.reset_states(x.shape[0])\n",
    "        for i in range(0, x.shape[1], num_samples):\n",
    "            wavs_batch = x[:, i:i+num_samples]\n",
    "            out_chunk = self.__call__(wavs_batch, sr)\n",
    "            outs.append(out_chunk)\n",
    "\n",
    "        stacked = torch.cat(outs, dim=1)\n",
    "        return stacked.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30680da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validator():\n",
    "    def __init__(self, url, force_onnx_cpu):\n",
    "        self.onnx = True if url.endswith('.onnx') else False\n",
    "        torch.hub.download_url_to_file(url, 'inf.model')\n",
    "        if self.onnx:\n",
    "            import onnxruntime\n",
    "            if force_onnx_cpu and 'CPUExecutionProvider' in onnxruntime.get_available_providers():\n",
    "                self.model = onnxruntime.InferenceSession('inf.model', providers=['CPUExecutionProvider'])\n",
    "            else:\n",
    "                self.model = onnxruntime.InferenceSession('inf.model')\n",
    "        else:\n",
    "            self.model = init_jit_model(model_path='inf.model')\n",
    "\n",
    "    def __call__(self, inputs: torch.Tensor):\n",
    "        with torch.no_grad():\n",
    "            if self.onnx:\n",
    "                ort_inputs = {'input': inputs.cpu().numpy()}\n",
    "                outs = self.model.run(None, ort_inputs)\n",
    "                outs = [torch.Tensor(x) for x in outs]\n",
    "            else:\n",
    "                outs = self.model(inputs)\n",
    "\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a5ae4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(path: str,\n",
    "               sampling_rate: int = 16000):\n",
    "\n",
    "    wav, sr = torchaudio.load(path)\n",
    "\n",
    "    if wav.size(0) > 1:\n",
    "        wav = wav.mean(dim=0, keepdim=True)\n",
    "\n",
    "    if sr != sampling_rate:\n",
    "        transform = torchaudio.transforms.Resample(orig_freq=sr,\n",
    "                                                   new_freq=sampling_rate)\n",
    "        wav = transform(wav)\n",
    "        sr = sampling_rate\n",
    "\n",
    "    assert sr == sampling_rate\n",
    "    return wav.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68566c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_audio(path: str,\n",
    "               tensor: torch.Tensor,\n",
    "               sampling_rate: int = 16000):\n",
    "    torchaudio.save(path, tensor.unsqueeze(0), sampling_rate, bits_per_sample=16)\n",
    "\n",
    "\n",
    "def init_jit_model(model_path: str,\n",
    "                   device=torch.device('cpu')):\n",
    "    torch.set_grad_enabled(False)\n",
    "    model = torch.jit.load(model_path, map_location=device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feee4222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_visualization(probs, step):\n",
    "    import pandas as pd\n",
    "    pd.DataFrame({'probs': probs},\n",
    "                 index=[x * step for x in range(len(probs))]).plot(figsize=(16, 8),\n",
    "                 kind='area', ylim=[0, 1.05], xlim=[0, len(probs) * step],\n",
    "                 xlabel='seconds',\n",
    "                 ylabel='speech probability',\n",
    "                 colormap='tab20')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52998a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speech_timestamps(audio: torch.Tensor,\n",
    "                          model,\n",
    "                          threshold: float = 0.5,\n",
    "                          sampling_rate: int = 16000,\n",
    "                          min_speech_duration_ms: int = 250,\n",
    "                          max_speech_duration_s: float = float('inf'),\n",
    "                          min_silence_duration_ms: int = 100,\n",
    "                          window_size_samples: int = 512,\n",
    "                          speech_pad_ms: int = 30,\n",
    "                          return_seconds: bool = False,\n",
    "                          visualize_probs: bool = False,\n",
    "                          progress_tracking_callback: Callable[[float], None] = None):\n",
    "\n",
    "    \"\"\"\n",
    "    This method is used for splitting long audios into speech chunks using silero VAD\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio: torch.Tensor, one dimensional\n",
    "        One dimensional float torch.Tensor, other types are casted to torch if possible\n",
    "\n",
    "    model: preloaded .jit silero VAD model\n",
    "\n",
    "    threshold: float (default - 0.5)\n",
    "        Speech threshold. Silero VAD outputs speech probabilities for each audio chunk, probabilities ABOVE this value are considered as SPEECH.\n",
    "        It is better to tune this parameter for each dataset separately, but \"lazy\" 0.5 is pretty good for most datasets.\n",
    "\n",
    "    sampling_rate: int (default - 16000)\n",
    "        Currently silero VAD models support 8000 and 16000 sample rates\n",
    "\n",
    "    min_speech_duration_ms: int (default - 250 milliseconds)\n",
    "        Final speech chunks shorter min_speech_duration_ms are thrown out\n",
    "\n",
    "    max_speech_duration_s: int (default -  inf)\n",
    "        Maximum duration of speech chunks in seconds\n",
    "        Chunks longer than max_speech_duration_s will be split at the timestamp of the last silence that lasts more than 100ms (if any), to prevent agressive cutting.\n",
    "        Otherwise, they will be split aggressively just before max_speech_duration_s.\n",
    "\n",
    "    min_silence_duration_ms: int (default - 100 milliseconds)\n",
    "        In the end of each speech chunk wait for min_silence_duration_ms before separating it\n",
    "\n",
    "    window_size_samples: int (default - 1536 samples)\n",
    "        Audio chunks of window_size_samples size are fed to the silero VAD model.\n",
    "        WARNING! Silero VAD models were trained using 512, 1024, 1536 samples for 16000 sample rate and 256, 512, 768 samples for 8000 sample rate.\n",
    "        Values other than these may affect model perfomance!!\n",
    "\n",
    "    speech_pad_ms: int (default - 30 milliseconds)\n",
    "        Final speech chunks are padded by speech_pad_ms each side\n",
    "\n",
    "    return_seconds: bool (default - False)\n",
    "        whether return timestamps in seconds (default - samples)\n",
    "\n",
    "    visualize_probs: bool (default - False)\n",
    "        whether draw prob hist or not\n",
    "\n",
    "    progress_tracking_callback: Callable[[float], None] (default - None)\n",
    "        callback function taking progress in percents as an argument\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    speeches: list of dicts\n",
    "        list containing ends and beginnings of speech chunks (samples or seconds based on return_seconds)\n",
    "    \"\"\"\n",
    "\n",
    "    if not torch.is_tensor(audio):\n",
    "        try:\n",
    "            audio = torch.Tensor(audio)\n",
    "        except:\n",
    "            raise TypeError(\"Audio cannot be casted to tensor. Cast it manually\")\n",
    "\n",
    "    if len(audio.shape) > 1:\n",
    "        for i in range(len(audio.shape)):  # trying to squeeze empty dimensions\n",
    "            audio = audio.squeeze(0)\n",
    "        if len(audio.shape) > 1:\n",
    "            raise ValueError(\"More than one dimension in audio. Are you trying to process audio with 2 channels?\")\n",
    "\n",
    "    if sampling_rate > 16000 and (sampling_rate % 16000 == 0):\n",
    "        step = sampling_rate // 16000\n",
    "        sampling_rate = 16000\n",
    "        audio = audio[::step]\n",
    "        warnings.warn('Sampling rate is a multiply of 16000, casting to 16000 manually!')\n",
    "    else:\n",
    "        step = 1\n",
    "\n",
    "    if sampling_rate == 8000 and window_size_samples > 768:\n",
    "        warnings.warn('window_size_samples is too big for 8000 sampling_rate! Better set window_size_samples to 256, 512 or 768 for 8000 sample rate!')\n",
    "    if window_size_samples not in [256, 512, 768, 1024, 1536]:\n",
    "        warnings.warn('Unusual window_size_samples! Supported window_size_samples:\\n - [512, 1024, 1536] for 16000 sampling_rate\\n - [256, 512, 768] for 8000 sampling_rate')\n",
    "\n",
    "    model.reset_states()\n",
    "    min_speech_samples = sampling_rate * min_speech_duration_ms / 1000\n",
    "    speech_pad_samples = sampling_rate * speech_pad_ms / 1000\n",
    "    max_speech_samples = sampling_rate * max_speech_duration_s - window_size_samples - 2 * speech_pad_samples\n",
    "    min_silence_samples = sampling_rate * min_silence_duration_ms / 1000\n",
    "    min_silence_samples_at_max_speech = sampling_rate * 98 / 1000\n",
    "\n",
    "    audio_length_samples = len(audio)\n",
    "\n",
    "    speech_probs = []\n",
    "    for current_start_sample in range(0, audio_length_samples, window_size_samples):\n",
    "        chunk = audio[current_start_sample: current_start_sample + window_size_samples]\n",
    "        if len(chunk) < window_size_samples:\n",
    "            chunk = torch.nn.functional.pad(chunk, (0, int(window_size_samples - len(chunk))))\n",
    "        speech_prob = model(chunk, sampling_rate).item()\n",
    "        speech_probs.append(speech_prob)\n",
    "        # caculate progress and seng it to callback function\n",
    "        progress = current_start_sample + window_size_samples\n",
    "        if progress > audio_length_samples:\n",
    "            progress = audio_length_samples\n",
    "        progress_percent = (progress / audio_length_samples) * 100\n",
    "        if progress_tracking_callback:\n",
    "            progress_tracking_callback(progress_percent)\n",
    "\n",
    "    triggered = False\n",
    "    speeches = []\n",
    "    current_speech = {}\n",
    "    neg_threshold = threshold - 0.15\n",
    "    temp_end = 0 # to save potential segment end (and tolerate some silence)\n",
    "    prev_end = next_start = 0 # to save potential segment limits in case of maximum segment size reached\n",
    "\n",
    "    for i, speech_prob in enumerate(speech_probs):\n",
    "        if (speech_prob >= threshold) and temp_end:\n",
    "            temp_end = 0\n",
    "            if next_start < prev_end:\n",
    "               next_start = window_size_samples * i\n",
    "\n",
    "        if (speech_prob >= threshold) and not triggered:\n",
    "            triggered = True\n",
    "            current_speech['start'] = window_size_samples * i\n",
    "            continue\n",
    "\n",
    "        if triggered and (window_size_samples * i) - current_speech['start'] > max_speech_samples:\n",
    "            if prev_end:\n",
    "                current_speech['end'] = prev_end\n",
    "                speeches.append(current_speech)\n",
    "                current_speech = {}\n",
    "                if next_start < prev_end: # previously reached silence (< neg_thres) and is still not speech (< thres)\n",
    "                    triggered = False\n",
    "                else:\n",
    "                    current_speech['start'] = next_start\n",
    "                prev_end = next_start = temp_end = 0\n",
    "            else:\n",
    "                current_speech['end'] = window_size_samples * i\n",
    "                speeches.append(current_speech)\n",
    "                current_speech = {}\n",
    "                prev_end = next_start = temp_end = 0\n",
    "                triggered = False\n",
    "                continue\n",
    "\n",
    "        if (speech_prob < neg_threshold) and triggered:\n",
    "            if not temp_end:\n",
    "                temp_end = window_size_samples * i\n",
    "            if ((window_size_samples * i) - temp_end) > min_silence_samples_at_max_speech : # condition to avoid cutting in very short silence\n",
    "                prev_end = temp_end\n",
    "            if (window_size_samples * i) - temp_end < min_silence_samples:\n",
    "                continue\n",
    "            else:\n",
    "                current_speech['end'] = temp_end\n",
    "                if (current_speech['end'] - current_speech['start']) > min_speech_samples:\n",
    "                    speeches.append(current_speech)\n",
    "                current_speech = {}\n",
    "                prev_end = next_start = temp_end = 0\n",
    "                triggered = False\n",
    "                continue\n",
    "\n",
    "    if current_speech and (audio_length_samples - current_speech['start']) > min_speech_samples:\n",
    "        current_speech['end'] = audio_length_samples\n",
    "        speeches.append(current_speech)\n",
    "\n",
    "    for i, speech in enumerate(speeches):\n",
    "        if i == 0:\n",
    "            speech['start'] = int(max(0, speech['start'] - speech_pad_samples))\n",
    "        if i != len(speeches) - 1:\n",
    "            silence_duration = speeches[i+1]['start'] - speech['end']\n",
    "            if silence_duration < 2 * speech_pad_samples:\n",
    "                speech['end'] += int(silence_duration // 2)\n",
    "                speeches[i+1]['start'] = int(max(0, speeches[i+1]['start'] - silence_duration // 2))\n",
    "            else:\n",
    "                speech['end'] = int(min(audio_length_samples, speech['end'] + speech_pad_samples))\n",
    "                speeches[i+1]['start'] = int(max(0, speeches[i+1]['start'] - speech_pad_samples))\n",
    "        else:\n",
    "            speech['end'] = int(min(audio_length_samples, speech['end'] + speech_pad_samples))\n",
    "\n",
    "    if return_seconds:\n",
    "        for speech_dict in speeches:\n",
    "            speech_dict['start'] = round(speech_dict['start'] / sampling_rate, 1)\n",
    "            speech_dict['end'] = round(speech_dict['end'] / sampling_rate, 1)\n",
    "    elif step > 1:\n",
    "        for speech_dict in speeches:\n",
    "            speech_dict['start'] *= step\n",
    "            speech_dict['end'] *= step\n",
    "\n",
    "    if visualize_probs:\n",
    "        make_visualization(speech_probs, window_size_samples / sampling_rate)\n",
    "\n",
    "    return speeches\n",
    "\n",
    "\n",
    "def get_number_ts(wav: torch.Tensor,\n",
    "                  model,\n",
    "                  model_stride=8,\n",
    "                  hop_length=160,\n",
    "                  sample_rate=16000):\n",
    "    wav = torch.unsqueeze(wav, dim=0)\n",
    "    perframe_logits = model(wav)[0]\n",
    "    perframe_preds = torch.argmax(torch.softmax(perframe_logits, dim=1), dim=1).squeeze()   # (1, num_frames_strided)\n",
    "    extended_preds = []\n",
    "    for i in perframe_preds:\n",
    "        extended_preds.extend([i.item()] * model_stride)\n",
    "    # len(extended_preds) is *num_frames_real*; for each frame of audio we know if it has a number in it.\n",
    "    triggered = False\n",
    "    timings = []\n",
    "    cur_timing = {}\n",
    "    for i, pred in enumerate(extended_preds):\n",
    "        if pred == 1:\n",
    "            if not triggered:\n",
    "                cur_timing['start'] = int((i * hop_length) / (sample_rate / 1000))\n",
    "                triggered = True\n",
    "        elif pred == 0:\n",
    "            if triggered:\n",
    "                cur_timing['end'] = int((i * hop_length) / (sample_rate / 1000))\n",
    "                timings.append(cur_timing)\n",
    "                cur_timing = {}\n",
    "                triggered = False\n",
    "    if cur_timing:\n",
    "        cur_timing['end'] = int(len(wav) / (sample_rate / 1000))\n",
    "        timings.append(cur_timing)\n",
    "    return timings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cf2de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language(wav: torch.Tensor,\n",
    "                 model):\n",
    "    wav = torch.unsqueeze(wav, dim=0)\n",
    "    lang_logits = model(wav)[2]\n",
    "    lang_pred = torch.argmax(torch.softmax(lang_logits, dim=1), dim=1).item()   # from 0 to len(languages) - 1\n",
    "    assert lang_pred < len(languages)\n",
    "    return languages[lang_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8914910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_and_group(wav: torch.Tensor,\n",
    "                           model,\n",
    "                           lang_dict: dict,\n",
    "                           lang_group_dict: dict,\n",
    "                           top_n=1):\n",
    "    wav = torch.unsqueeze(wav, dim=0)\n",
    "    lang_logits, lang_group_logits = model(wav)\n",
    "\n",
    "    softm = torch.softmax(lang_logits, dim=1).squeeze()\n",
    "    softm_group = torch.softmax(lang_group_logits, dim=1).squeeze()\n",
    "\n",
    "    srtd = torch.argsort(softm, descending=True)\n",
    "    srtd_group = torch.argsort(softm_group, descending=True)\n",
    "\n",
    "    outs = []\n",
    "    outs_group = []\n",
    "    for i in range(top_n):\n",
    "        prob = round(softm[srtd[i]].item(), 2)\n",
    "        prob_group = round(softm_group[srtd_group[i]].item(), 2)\n",
    "        outs.append((lang_dict[str(srtd[i].item())], prob))\n",
    "        outs_group.append((lang_group_dict[str(srtd_group[i].item())], prob_group))\n",
    "\n",
    "    return outs, outs_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89e31b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VADIterator:\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 threshold: float = 0.5,\n",
    "                 sampling_rate: int = 16000,\n",
    "                 min_silence_duration_ms: int = 100,\n",
    "                 speech_pad_ms: int = 30\n",
    "                 ):\n",
    "\n",
    "        \"\"\"\n",
    "        Class for stream imitation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model: preloaded .jit silero VAD model\n",
    "\n",
    "        threshold: float (default - 0.5)\n",
    "            Speech threshold. Silero VAD outputs speech probabilities for each audio chunk, probabilities ABOVE this value are considered as SPEECH.\n",
    "            It is better to tune this parameter for each dataset separately, but \"lazy\" 0.5 is pretty good for most datasets.\n",
    "\n",
    "        sampling_rate: int (default - 16000)\n",
    "            Currently silero VAD models support 8000 and 16000 sample rates\n",
    "\n",
    "        min_silence_duration_ms: int (default - 100 milliseconds)\n",
    "            In the end of each speech chunk wait for min_silence_duration_ms before separating it\n",
    "\n",
    "        speech_pad_ms: int (default - 30 milliseconds)\n",
    "            Final speech chunks are padded by speech_pad_ms each side\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = model\n",
    "        self.threshold = threshold\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "        if sampling_rate not in [8000, 16000]:\n",
    "            raise ValueError('VADIterator does not support sampling rates other than [8000, 16000]')\n",
    "\n",
    "        self.min_silence_samples = sampling_rate * min_silence_duration_ms / 1000\n",
    "        self.speech_pad_samples = sampling_rate * speech_pad_ms / 1000\n",
    "        self.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "178ba501",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def reset_states(self):\n",
    "\n",
    "        self.model.reset_states()\n",
    "        self.triggered = False\n",
    "        self.temp_end = 0\n",
    "        self.current_sample = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4afa1bef",
   "metadata": {},
   "outputs": [],
   "source": [
    " def __call__(self, x, return_seconds=False):\n",
    "        \"\"\"\n",
    "        x: torch.Tensor\n",
    "            audio chunk (see examples in repo)\n",
    "\n",
    "        return_seconds: bool (default - False)\n",
    "            whether return timestamps in seconds (default - samples)\n",
    "        \"\"\"\n",
    "\n",
    "        if not torch.is_tensor(x):\n",
    "            try:\n",
    "                x = torch.Tensor(x)\n",
    "            except:\n",
    "                raise TypeError(\"Audio cannot be casted to tensor. Cast it manually\")\n",
    "\n",
    "        window_size_samples = len(x[0]) if x.dim() == 2 else len(x)\n",
    "        self.current_sample += window_size_samples\n",
    "\n",
    "        speech_prob = self.model(x, self.sampling_rate).item()\n",
    "\n",
    "        if (speech_prob >= self.threshold) and self.temp_end:\n",
    "            self.temp_end = 0\n",
    "\n",
    "        if (speech_prob >= self.threshold) and not self.triggered:\n",
    "            self.triggered = True\n",
    "            speech_start = self.current_sample - self.speech_pad_samples\n",
    "            return {'start': int(speech_start) if not return_seconds else round(speech_start / self.sampling_rate, 1)}\n",
    "\n",
    "        if (speech_prob < self.threshold - 0.15) and self.triggered:\n",
    "            if not self.temp_end:\n",
    "                self.temp_end = self.current_sample\n",
    "            if self.current_sample - self.temp_end < self.min_silence_samples:\n",
    "                return None\n",
    "            else:\n",
    "                speech_end = self.temp_end + self.speech_pad_samples\n",
    "                self.temp_end = 0\n",
    "                self.triggered = False\n",
    "                return {'end': int(speech_end) if not return_seconds else round(speech_end / self.sampling_rate, 1)}\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a6f7f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_chunks(tss: List[dict],\n",
    "                   wav: torch.Tensor):\n",
    "    chunks = []\n",
    "    for i in tss:\n",
    "        chunks.append(wav[i['start']: i['end']])\n",
    "    return torch.cat(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83b9200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_chunks(tss: List[dict],\n",
    "                wav: torch.Tensor):\n",
    "    chunks = []\n",
    "    cur_start = 0\n",
    "    for i in tss:\n",
    "        chunks.append((wav[cur_start: i['start']]))\n",
    "        cur_start = i['end']\n",
    "    return torch.cat(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e732cf51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
